import os
import sys
import json
import torch
from fastapi import FastAPI, Request, HTTPException
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from opencc import OpenCC  # ç¹é«”è½‰æ›

# --- RAG èˆ‡ æª¢ç´¢ ç›¸é—œå¥—ä»¶ ---
# --- æ¨™æº–åŒ– Import (è«‹ç›´æ¥è¦†è“‹èˆŠçš„ Import å€å¡Š) ---
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
# åœ¨ LangChain 0.3.0+ï¼ŒEnsembleRetriever ä½æ–¼æ¨™æº–è·¯å¾‘
from langchain.retrievers import EnsembleRetriever

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# 2. å–å¾—å¯†é‘°èˆ‡è¨­å®š
channel_secret = os.getenv('LINE_CHANNEL_SECRET') 
channel_access_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
hf_token = os.getenv('HF_TOKEN')

if not all([channel_secret, channel_access_token, hf_token]):
    print("éŒ¯èª¤ï¼šè«‹ç¢ºèª .env æª”æ¡ˆä¸­å·²è¨­å®š LINE å¯†é‘°èˆ‡ HF_TOKEN")
    sys.exit(1)

# 3. åˆå§‹åŒ– LINE Bot API
line_bot_api = LineBotApi(channel_access_token)
handler = WebhookHandler(channel_secret)

# ==========================================
# [AI æ•™æˆé‡é»æ•™å­¸] RAG æ··åˆæª¢ç´¢ç³»çµ±åˆå§‹åŒ–
# ==========================================
print("æ­£åœ¨å»ºç«‹ RAG æ··åˆæª¢ç´¢ç³»çµ± (FAISS + BM25)...")

ensemble_retriever = None  # å…¨åŸŸè®Šæ•¸

try:
    # A. è®€å– knowledge.jsonl ä¸¦é€²è¡Œçµæ§‹åŒ–è™•ç†
    documents = []
    
    if not os.path.exists("knowledge.jsonl"):
        print("è­¦å‘Šï¼šæ‰¾ä¸åˆ° knowledge.jsonlï¼Œå°‡ä½¿ç”¨ç©ºè³‡æ–™ã€‚")
    else:
        with open("knowledge.jsonl", "r", encoding="utf-8") as f:
            for line_number, line in enumerate(f, 1):
                line = line.strip()
                if not line: continue
                
                try:
                    data = json.loads(line)
                    # çµ„åˆå‡ºä¹¾æ·¨çš„æ–‡å­—æ ¼å¼
                    instruction = data.get('symptom', data.get('instruction', ''))
                    response = data.get('solution', data.get('response', ''))
                    page_content = f"æ•…éšœç—‡ç‹€ï¼š{instruction}\næ’é™¤æ–¹æ³•ï¼š{response}"
                    
                    doc = Document(
                        page_content=page_content,
                        metadata={"source": "knowledge.jsonl", "row": line_number}
                    )
                    documents.append(doc)
                except json.JSONDecodeError:
                    print(f"è­¦å‘Šï¼šç¬¬ {line_number} è¡Œæ ¼å¼éŒ¯èª¤ï¼Œå·²è·³éã€‚")

    docs = documents
    print(f"æˆåŠŸè¼‰å…¥ {len(docs)} æ¢çŸ¥è­˜ç‰‡æ®µï¼")

    if docs:
        # B. è¼‰å…¥ Embedding æ¨¡å‹ (ç”¨æ–¼èªæ„ç†è§£)
        # ä½¿ç”¨å¤šèªè¨€æ¨¡å‹ä»¥æ”¯æ´ä¸­æ–‡èªæ„
        # embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

        # ä¿®æ”¹å¾Œ (æ”¹ç”¨ BGE-M3ï¼Œç›®å‰ä¸­æ–‡æª¢ç´¢çš„æœ€å¼·è€…ä¹‹ä¸€)
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

        # C. å»ºç«‹å…©ç¨®æª¢ç´¢å™¨
        # 1. BM25 (é—œéµå­—ç²¾æº–æª¢ç´¢) - å°ˆæ²» "EB1", "ç¸½æ•…éšœç‡ˆ" é€™ç¨®å°ˆæœ‰åè©
        bm25_retriever = BM25Retriever.from_documents(docs)
        bm25_retriever.k = 3  # å–å‰ 3 å

        # 2. FAISS (èªæ„å‘é‡æª¢ç´¢) - å°ˆæ²» "è»Šå­ä¸å‹•", "æ²’é›»" é€™ç¨®æ¨¡ç³Šæè¿°
        vector_db = FAISS.from_documents(docs, embeddings)
        faiss_retriever = vector_db.as_retriever(search_kwargs={"k": 3})

        # D. å»ºç«‹ Ensemble (æ··åˆ) æª¢ç´¢å™¨
        # weights=[0.5, 0.5] ä»£è¡¨é—œéµå­—å’Œèªæ„åŒæ¨£é‡è¦
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, faiss_retriever],
            weights=[0.5, 0.5]
        )
        print("æ··åˆæª¢ç´¢ç³»çµ± (Hybrid Search) å»ºç«‹å®Œæˆï¼")
    else:
        print("è­¦å‘Šï¼šæ²’æœ‰è¼‰å…¥ä»»ä½•æ–‡ä»¶ï¼ŒçŸ¥è­˜åº«ç‚ºç©ºã€‚")

except Exception as e:
    print(f"RAG åˆå§‹åŒ–å¤±æ•—: {e}")
    sys.exit(1)

# ==========================================
# åˆå§‹åŒ– LLM æ¨¡å‹
# ==========================================
# å»ºè­°ï¼šè‹¥è¨˜æ†¶é«”å…è¨±ï¼Œå°‡æ­¤è™•æ”¹ç‚º "Qwen/Qwen2.5-1.5B-Instruct" æ•ˆæœæœƒæ›´å¥½
MODEL_ID = "google/gemma-3-270m-it" 

print(f"æ­£åœ¨è¼‰å…¥ç”Ÿæˆæ¨¡å‹ {MODEL_ID} ...")
try:
    if torch.backends.mps.is_available():
        device = "mps"
        print("ğŸš€ å•Ÿå‹• Mac GPU ç¡¬é«”åŠ é€Ÿ (MPS)")
    else:
        device = "cpu"
        print("ğŸ¢ ä½¿ç”¨ CPU æ¨¡å¼")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        token=hf_token,
        torch_dtype=torch.float32, 
        low_cpu_mem_usage=True
    ).to(device)
    
    print(f"ç”Ÿæˆæ¨¡å‹è¼‰å…¥å®Œæˆï¼é‹è¡Œè£ç½®: {device}")
except Exception as e:
    print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    sys.exit(1)

app = FastAPI()

# 4. è¨­å®š Webhook å…¥å£
@app.post("/callback")
async def callback(request: Request):
    signature = request.headers['X-Line-Signature']
    body = await request.body()
    try:
        handler.handle(body.decode(), signature)
    except InvalidSignatureError:
        raise HTTPException(status_code=400, detail="Invalid signature")
    return 'OK'

# 5. è™•ç†æ–‡å­—è¨Šæ¯çš„é‚è¼¯
@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    user_msg = event.message.text
    print(f"æ”¶åˆ°è¨Šæ¯: {user_msg}")

    try:
        # --- [RAG æ··åˆæª¢ç´¢éšæ®µ] ---
        rag_context = ""
        found_docs = []

        if ensemble_retriever:
            # ä½¿ç”¨ invoke é€²è¡Œæ··åˆæœå°‹
            found_docs = ensemble_retriever.invoke(user_msg)
            
            if found_docs:
                # ç‚ºäº†é¿å… Context å¤ªé•·ï¼Œåªå–å‰ 2 ç­†æœ€ç›¸é—œçš„
                top_docs = found_docs[:2]
                rag_context = "\n\n".join([f"ã€åƒè€ƒè³‡æ–™ {i+1}ã€‘:\n{doc.page_content}" for i, doc in enumerate(top_docs)])
                
                # [é™¤éŒ¯ Log] å°å‡ºæ‰¾åˆ°ä»€éº¼ï¼Œç¢ºèª BM25 æ˜¯å¦ç”Ÿæ•ˆ
                print(f"--- ğŸ” æª¢ç´¢åˆ°çš„çŸ¥è­˜ (Top 2) ---")
                for doc in top_docs:
                    print(f"[å…§å®¹]: {doc.page_content[:50]}...")
                print("-----------------------------")

        # --- [Prompt çµ„åˆéšæ®µ] ---
        # æ•™æˆä¿®æ­£ï¼šä½¿ç”¨æ›´åš´è¬¹çš„æŒ‡ä»¤æ ¼å¼ï¼Œé˜²æ­¢æ¨¡å‹çæ°æˆ–æ¥é¾
        if rag_context:
            full_prompt_msg = (
                f"### æŒ‡ä»¤ ###\n"
                f"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ·é‹ç¶­ä¿®å°ˆå®¶ã€‚è«‹ä¾æ“šä¸‹æ–¹æä¾›çš„ã€ç¶­ä¿®æ‰‹å†Šç‰‡æ®µã€‘ï¼Œå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚\n"
                f"è¦å‰‡ 1ï¼šè«‹ç›´æ¥åˆ—å‡ºæ’é™¤æ­¥é©Ÿï¼Œä¸è¦å»¢è©±ã€‚\n"
                f"è¦å‰‡ 2ï¼šè‹¥æ‰‹å†Šå…§å®¹èˆ‡å•é¡Œç„¡é—œï¼Œè«‹ç›´æ¥å›ç­”ã€ŒæŸ¥ç„¡ç›¸é—œç¶­ä¿®è³‡æ–™ã€ï¼Œä¸å¯è‡ªè¡Œç·¨é€ ã€‚\n\n"
                f"### ç¶­ä¿®æ‰‹å†Šç‰‡æ®µ ###\n{rag_context}\n\n"
                f"### ä½¿ç”¨è€…å•é¡Œ ###\n{user_msg}\n\n"
                f"### ä½ çš„å°ˆæ¥­å›ç­” ###\n"
            )
        else:
            full_prompt_msg = f"ä½ æ˜¯ä¸€ä½åŠ©ç†ã€‚è«‹å›ç­”å•é¡Œï¼š{user_msg}"

        # --- [LLM ç”Ÿæˆéšæ®µ] ---
        chat = [
            { "role": "user", "content": full_prompt_msg },
        ]
        prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

        inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(device)
        input_length = inputs.input_ids.shape[1]
        
        streamer = TextStreamer(tokenizer, skip_prompt=True)
        
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=300,      # ä¸éœ€è¦å¤ªé•·ï¼Œç¶­ä¿®æ­¥é©Ÿé€šå¸¸å¾ˆç°¡æ½”
            repetition_penalty=1.2,  # æé«˜æ‡²ç½°ï¼Œé¿å…é‡è¤‡
            do_sample=True,
            temperature=0.1,         # é™ä½éš¨æ©Ÿæ€§ï¼Œè®“å›ç­”æ›´æ­»æ¿ã€ç²¾ç¢º
            streamer=streamer
        )

        generated_tokens = outputs[0][input_length:]
        final_reply = tokenizer.decode(generated_tokens, skip_special_tokens=True)

        # --- [å¾Œè™•ç†éšæ®µ] ---
        cc = OpenCC('s2t')
        final_reply = cc.convert(final_reply)
        final_reply = final_reply.replace("**", "").replace("###", "").strip()

        if not final_reply:
            final_reply = "æŠ±æ­‰ï¼Œç³»çµ±é‹ç®—ä¸­ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

    except Exception as e:
        print(f"ç”ŸæˆéŒ¯èª¤: {e}")
        final_reply = "æŠ±æ­‰ï¼Œç³»çµ±ç™¼ç”ŸéŒ¯èª¤ã€‚"

    # å›è¦†è¨Šæ¯
    line_bot_api.reply_message(
        event.reply_token,
        TextSendMessage(text=final_reply)
    )

if __name__ == "__main__":
    import uvicorn
    # å•Ÿå‹•ä¼ºæœå™¨
    uvicorn.run(app, host="0.0.0.0", port=5000)