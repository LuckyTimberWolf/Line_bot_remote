import os
import sys
import torch
from fastapi import FastAPI, Request, HTTPException
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM
from opencc import OpenCC  # ç¹é«”è½‰æ›

# --- RAG ç›¸é—œå¥—ä»¶ ---
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# 2. å–å¾—å¯†é‘°èˆ‡è¨­å®š
channel_secret = os.getenv('LINE_CHANNEL_SECRET') 
channel_access_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
hf_token = os.getenv('HF_TOKEN')

if not all([channel_secret, channel_access_token, hf_token]):
    print("éŒ¯èª¤ï¼šè«‹ç¢ºèª .env æª”æ¡ˆä¸­å·²è¨­å®š LINE å¯†é‘°èˆ‡ HF_TOKEN")
    sys.exit(1)

# 3. åˆå§‹åŒ– LINE Bot API
line_bot_api = LineBotApi(channel_access_token)
handler = WebhookHandler(channel_secret)

# ==========================================
# [AI æ•™æˆé‡é»æ•™å­¸] RAG çŸ¥è­˜åº«åˆå§‹åŒ–
# ==========================================
print("æ­£åœ¨å»ºç«‹ RAG çŸ¥è­˜åº« (FAISS)...")
try:
    # A. è®€å– knowledge.txt
    if not os.path.exists("knowledge.txt"):
        # å»ºç«‹ä¸€å€‹é è¨­æª”æ¡ˆé¿å…å ±éŒ¯
        with open("knowledge.txt", "w", encoding="utf-8") as f:
            f.write("é€™æ˜¯é è¨­çš„çŸ¥è­˜åº«å…§å®¹ã€‚ç›®å‰æ²’æœ‰ç‰¹å®šè³‡æ–™ã€‚")
    
    loader = TextLoader("knowledge.txt", encoding="utf-8")
    documents = loader.load()

    # B. åˆ‡åˆ†æ–‡å­— (é¿å…æ–‡ç« å¤ªé•·ï¼Œæ¨¡å‹åƒä¸ä¸‹)

    text_splitter = CharacterTextSplitter(
            separator="},",   # ä¾ç…§ JSON ç‰©ä»¶çš„é€—è™Ÿåˆ‡åˆ†
            chunk_size=300, # æ¯æ®µ 300 å­—å…ƒ
            chunk_overlap=0 # ä¸é‡ç–Š
    )
    #text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    docs = text_splitter.split_documents(documents)

    # C. è¼‰å…¥ Embedding æ¨¡å‹ (è² è²¬æŠŠæ–‡å­—è®Šæˆå‘é‡)
    # ä½¿ç”¨è¼•é‡ç´šçš„ sentence-transformersï¼Œé©åˆæœ¬æ©Ÿé‹è¡Œ
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # D. å»ºç«‹å‘é‡è³‡æ–™åº«
    vector_db = FAISS.from_documents(docs, embeddings)
    print("çŸ¥è­˜åº«å»ºç«‹å®Œæˆï¼")

except Exception as e:
    print(f"RAG åˆå§‹åŒ–å¤±æ•—: {e}")
    # è‹¥å¤±æ•—ï¼Œç‚ºäº†ä¸è®“ç¨‹å¼æ›æ‰ï¼Œè¨­ç‚º None
    vector_db = None

# ==========================================
# åˆå§‹åŒ– LLM æ¨¡å‹ (Gemma)
# ==========================================
MODEL_ID = "google/gemma-3-270m-it"

print(f"æ­£åœ¨è¼‰å…¥ç”Ÿæˆæ¨¡å‹ {MODEL_ID} ...")
try:
    device = "cpu" # é è¨­ç‚º CPU
    '''
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if torch.backends.mps.is_available(): # æ”¯æ´ Mac M1/M2/M3
        device = "mps"
    '''
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        token=hf_token,
        torch_dtype=torch.float32,  # <--- é—œéµä¿®æ”¹ï¼šå¼·åˆ¶å…¨ç²¾åº¦ï¼Œç©©å¦‚æ³°å±±
        #torch_dtype=torch.float16 if device != "cpu" else torch.float32,
        low_cpu_mem_usage=True
    ).to(device)
    print(f"ç”Ÿæˆæ¨¡å‹è¼‰å…¥å®Œæˆï¼é‹è¡Œè£ç½®: {device}")
except Exception as e:
    print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    sys.exit(1)

app = FastAPI()

# 4. è¨­å®š Webhook å…¥å£
@app.post("/callback")
async def callback(request: Request):
    signature = request.headers['X-Line-Signature']
    body = await request.body()
    try:
        handler.handle(body.decode(), signature)
    except InvalidSignatureError:
        raise HTTPException(status_code=400, detail="Invalid signature")
    return 'OK'

# 5. è™•ç†æ–‡å­—è¨Šæ¯çš„é‚è¼¯
@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    user_msg = event.message.text
    print(f"æ”¶åˆ°è¨Šæ¯: {user_msg}")

    try:
        # --- [RAG æª¢ç´¢éšæ®µ] ---
        rag_context = ""
        if vector_db:
            # æœå°‹æœ€ç›¸é—œçš„ 2 æ®µæ–‡å­— (å¢åŠ  k å¯ä»¥è®“å®ƒè®€å¤šä¸€é»ï¼Œä½† 270M è¨˜æ†¶é«”æœ‰é™ï¼Œå…ˆç¶­æŒ 2)
            search_results = vector_db.similarity_search(user_msg, k=2)
            if search_results:
                rag_context = "\n".join([res.page_content for res in search_results])
                # ç‚ºäº†é™¤éŒ¯ï¼Œæˆ‘å€‘æŠŠå®ƒå°å‡ºä¾†çœ‹çœ‹åˆ°åº•æŠ“åˆ°äº†ä»€éº¼
                print(f"æœå°‹åˆ°çš„ç›¸é—œçŸ¥è­˜: {rag_context[:100]}...") 

        # --- [Prompt çµ„åˆéšæ®µ] ---
        # é‡å° 270M å°æ¨¡å‹çš„å„ªåŒ–æŒ‡ä»¤
        if rag_context:
            full_prompt_msg = (
                f"ä½ æ˜¯ä¸€ä½æ·é‹ç¶­ä¿®å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹ã€ç¶­ä¿®æ‰‹å†Šã€‘å›ç­”å•é¡Œã€‚\n"
                f"ã€ç¶­ä¿®æ‰‹å†Šã€‘ï¼š\n{rag_context}\n\n"
                f"å•é¡Œï¼š{user_msg}\n"
                f"å›ç­”ï¼š"
            )
        else:
            full_prompt_msg = (
                f"ä½ æ˜¯ä¸€ä½åŠ©ç†ã€‚è«‹å›ç­”å•é¡Œï¼š{user_msg}"
            )

        # --- [LLM ç”Ÿæˆéšæ®µ] (ä¿®å¾© Attention Mask è­¦å‘Š) ---
        chat = [
            { "role": "user", "content": full_prompt_msg },
        ]
        prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

        # âœ… ä¿®æ”¹é»ï¼šæ”¹ç”¨ tokenizer ç›´æ¥å›å‚³ tensorï¼Œä¸¦å–å¾— attention_mask
        inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(device)
        
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,  # <--- åŠ å…¥é€™è¡Œæ¶ˆé™¤è­¦å‘Š
            max_new_tokens=400  # é•·åº¦
        )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        final_reply = generated_text.split("model\n")[-1]

        # --- [å¾Œè™•ç†éšæ®µ] ç°¡è½‰ç¹ ---
        cc = OpenCC('s2t')
        final_reply = cc.convert(final_reply)
        final_reply = final_reply.replace("**", "")

        # ğŸ›‘ [æ•™æˆæ–°å¢] é˜²å‘†æ©Ÿåˆ¶ï¼šå¦‚æœæ¨¡å‹å›ç­”ç©ºç™½ï¼Œæ‰‹å‹•å¡ä¸€å¥è©±
        if not final_reply or final_reply.strip() == "":
            print("è­¦å‘Šï¼šæ¨¡å‹ç”Ÿæˆäº†ç©ºå­—ä¸²ï¼Œä½¿ç”¨é è¨­å›è¦†ã€‚")
            final_reply = "æŠ±æ­‰ï¼Œæˆ‘æ­£åœ¨æ€è€ƒä¸­ï¼Œä½†æš«æ™‚ç„¡æ³•ç”¢ç”Ÿå›æ‡‰ã€‚è«‹å†è©¦ä¸€æ¬¡æˆ–æä¾›æ›´å¤šè³‡è¨Šã€‚"

    except Exception as e:
        print(f"ç”ŸæˆéŒ¯èª¤: {e}")
        final_reply = "æŠ±æ­‰ï¼Œç³»çµ±ç™¼ç”ŸéŒ¯èª¤ã€‚"

    # å›è¦†è¨Šæ¯
    line_bot_api.reply_message(
        event.reply_token,
        TextSendMessage(text=final_reply)
    )