import os
import sys
import json  # å¿…é ˆåŒ¯å…¥ï¼Œç”¨æ–¼è§£æ JSONL
import torch
from fastapi import FastAPI, Request, HTTPException
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM
from opencc import OpenCC  # ç¹é«”è½‰æ›
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer # <--- æ–°å¢ TextStreamer

# --- RAG ç›¸é—œå¥—ä»¶ ---
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document  # é€™æ˜¯æ–°ç‰ˆçš„æ­£ç¢ºä½ç½®

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# 2. å–å¾—å¯†é‘°èˆ‡è¨­å®š
channel_secret = os.getenv('LINE_CHANNEL_SECRET') 
channel_access_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
hf_token = os.getenv('HF_TOKEN')

if not all([channel_secret, channel_access_token, hf_token]):
    print("éŒ¯èª¤ï¼šè«‹ç¢ºèª .env æª”æ¡ˆä¸­å·²è¨­å®š LINE å¯†é‘°èˆ‡ HF_TOKEN")
    sys.exit(1)

# 3. åˆå§‹åŒ– LINE Bot API
line_bot_api = LineBotApi(channel_access_token)
handler = WebhookHandler(channel_secret)

# ==========================================
# [AI æ•™æˆé‡é»æ•™å­¸] RAG çŸ¥è­˜åº«åˆå§‹åŒ–
# ==========================================
print("æ­£åœ¨å»ºç«‹ RAG çŸ¥è­˜åº« (FAISS)...")

try:
    # A. è®€å– knowledge.jsonl ä¸¦é€²è¡Œçµæ§‹åŒ–è™•ç†
    documents = []
    
    if not os.path.exists("knowledge.jsonl"):
        print("è­¦å‘Šï¼šæ‰¾ä¸åˆ° knowledge.jsonlï¼Œå°‡ä½¿ç”¨ç©ºè³‡æ–™ã€‚")
    else:
        with open("knowledge.jsonl", "r", encoding="utf-8") as f:
            for line_number, line in enumerate(f, 1):
                line = line.strip()
                if not line: continue  # è·³éç©ºè¡Œ
                
                try:
                    # å˜—è©¦è§£æ JSON
                    data = json.loads(line)
                    
                    # [æ•™æˆä¿®æ­£ 1]ï¼šç›¸å®¹å¤šç¨®æ¬„ä½åç¨± (symptom/solution æˆ– instruction/response)
                    instruction = data.get('symptom', data.get('instruction', ''))
                    response = data.get('solution', data.get('response', ''))
                    
                    # çµ„åˆå‡ºä¹¾æ·¨çš„æ–‡å­—æ ¼å¼ï¼Œå»é™¤ JSON ç¬¦è™Ÿå¹²æ“¾
                    page_content = f"æ•…éšœç—‡ç‹€ï¼š{instruction}\næ’é™¤æ–¹æ³•ï¼š{response}"
                    
                    # å»ºç«‹ LangChain çš„ Document ç‰©ä»¶
                    doc = Document(
                        page_content=page_content,
                        metadata={"source": "knowledge.jsonl", "row": line_number}
                    )
                    documents.append(doc)
                except json.JSONDecodeError:
                    print(f"è­¦å‘Šï¼šç¬¬ {line_number} è¡Œæ ¼å¼éŒ¯èª¤ï¼Œå·²è·³éã€‚")

    # B. è¨­å®šæ–‡ä»¶åˆ—è¡¨ (ä¸éœ€è¦åˆ‡åˆ† splitterï¼Œå› ç‚ºæ¯ä¸€è¡Œå·²ç¶“æ˜¯ç¨ç«‹çŸ¥è­˜é»)
    docs = documents
    print(f"æˆåŠŸè¼‰å…¥ {len(docs)} æ¢çŸ¥è­˜ç‰‡æ®µï¼")
    
    # æª¢æŸ¥ç¬¬ä¸€æ¢è³‡æ–™ç¢ºèªæ ¼å¼æ­£ç¢º (é™¤éŒ¯ç”¨)
    if docs:
        print(f"ç¯„ä¾‹è³‡æ–™ç‰‡æ®µ: {docs[0].page_content[:50]}...")

    # C. è¼‰å…¥ Embedding æ¨¡å‹
    #embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    # [æ•™æˆä¿®æ­£] æ”¹ç”¨å¤šèªè¨€æ¨¡å‹ï¼Œé€™æ¨£æ‰åˆ†å¾—æ¸…ã€Œç‰½å¼•ã€è·Ÿã€Œé›†é›»å¼“ã€çš„å·®åˆ¥
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    # D. å»ºç«‹å‘é‡è³‡æ–™åº«
    if docs:
        vector_db = FAISS.from_documents(docs, embeddings)
        print("çŸ¥è­˜åº«å»ºç«‹å®Œæˆï¼")
    else:
        print("è­¦å‘Šï¼šæ²’æœ‰è¼‰å…¥ä»»ä½•æ–‡ä»¶ï¼ŒçŸ¥è­˜åº«ç‚ºç©ºã€‚")
        vector_db = None

except Exception as e:
    print(f"RAG åˆå§‹åŒ–å¤±æ•—: {e}")
    vector_db = None

# ==========================================
# åˆå§‹åŒ– LLM æ¨¡å‹ (Gemma)
# ==========================================
#MODEL_ID = "google/gemma-3-270m-it"

# [æ•™æˆæ¨è–¦] æ”¹ç”¨ Qwen 2.5 (1.5B)ï¼Œä¸­æ–‡èƒ½åŠ›èˆ‡é‚è¼¯å¤§å¹…æå‡
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"

print(f"æ­£åœ¨è¼‰å…¥ç”Ÿæˆæ¨¡å‹ {MODEL_ID} ...")
try:
    # 1. å„ªå…ˆå˜—è©¦ä½¿ç”¨ MPS (Mac GPU)
    if torch.backends.mps.is_available():
        device = "mps"
        print("ğŸš€ å•Ÿå‹• Mac GPU ç¡¬é«”åŠ é€Ÿ (MPS)")
    else:
        device = "cpu"
        print("ğŸ¢ ä½¿ç”¨ CPU æ¨¡å¼")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)
    
    # 2. [é—œéµä¿®æ­£] å¼·åˆ¶ä½¿ç”¨ float32 è¼‰å…¥æ¨¡å‹
    # é€™èƒ½è§£æ±º Mac MPS å‡ºç¾ "probability tensor contains nan" çš„éŒ¯èª¤
    # é›–ç„¶æ¯” float16 ä½”è¨˜æ†¶é«”ï¼Œä½†æ¯” CPU å¿«éå¸¸å¤šä¸”ç©©å®š
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        token=hf_token,
        torch_dtype=torch.float32, 
        low_cpu_mem_usage=True
    ).to(device)
    
    print(f"ç”Ÿæˆæ¨¡å‹è¼‰å…¥å®Œæˆï¼é‹è¡Œè£ç½®: {device}")
except Exception as e:
    print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    sys.exit(1)

app = FastAPI()

# 4. è¨­å®š Webhook å…¥å£
@app.post("/callback")
async def callback(request: Request):
    signature = request.headers['X-Line-Signature']
    body = await request.body()
    try:
        handler.handle(body.decode(), signature)
    except InvalidSignatureError:
        raise HTTPException(status_code=400, detail="Invalid signature")
    return 'OK'

# 5. è™•ç†æ–‡å­—è¨Šæ¯çš„é‚è¼¯
@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    user_msg = event.message.text
    print(f"æ”¶åˆ°è¨Šæ¯: {user_msg}")

    try:
        # --- [RAG æª¢ç´¢éšæ®µ] ---
        rag_context = ""
        if vector_db:
            # æœå°‹æœ€ç›¸é—œçš„ 2 æ®µæ–‡å­—
            search_results = vector_db.similarity_search(user_msg, k=3)
            if search_results:
                rag_context = "\n".join([res.page_content for res in search_results])
                print(f"æœå°‹åˆ°çš„ç›¸é—œçŸ¥è­˜: {rag_context[:100]}...") 

        # --- [Prompt çµ„åˆéšæ®µ] ---
        if rag_context:
            full_prompt_msg = (
                f"ä½ æ˜¯ä¸€ä½æ·é‹ç¶­ä¿®å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹ã€ç¶­ä¿®æ‰‹å†Šã€‘å›ç­”å•é¡Œã€‚\n"
                f"ã€ç¶­ä¿®æ‰‹å†Šã€‘ï¼š\n{rag_context}\n\n"
                f"å•é¡Œï¼š{user_msg}\n"
                f"å›ç­”ï¼š"
            )
        else:
            full_prompt_msg = (
                f"ä½ æ˜¯ä¸€ä½åŠ©ç†ã€‚è«‹å›ç­”å•é¡Œï¼š{user_msg}"
            )

        # --- [LLM ç”Ÿæˆéšæ®µ] ---
        chat = [
            { "role": "user", "content": full_prompt_msg },
        ]
        prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

        inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(device)
        
        # [æ•™æˆä¿®æ­£ 3]ï¼šä½¿ç”¨ç²¾æº–åˆ‡å‰²æ³• (Input Length Slicing)
        input_length = inputs.input_ids.shape[1]
        streamer = TextStreamer(tokenizer, skip_prompt=True)
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=400,
            repetition_penalty=1.1,  # é˜²æ­¢é‡è¤‡
            do_sample=True,          # è®“å›ç­”ç¨å¾®è‡ªç„¶ä¸€é»
            temperature=0.3,          # é™ä½éš¨æ©Ÿæ€§ï¼Œå°ˆæ³¨æ–¼æ‰‹å†Š
            streamer=streamer        # <--- åŠ å…¥é€™ä¸€è¡Œ
        )

        # åªè§£ç¢¼ã€Œæ–°ç”Ÿæˆçš„ã€tokenï¼Œå¾¹åº•è§£æ±º split åˆ‡å‰²éŒ¯èª¤å•é¡Œ
        generated_tokens = outputs[0][input_length:]
        final_reply = tokenizer.decode(generated_tokens, skip_special_tokens=True)

        # --- [å¾Œè™•ç†éšæ®µ] ---
        cc = OpenCC('s2t')
        final_reply = cc.convert(final_reply)
        final_reply = final_reply.replace("**", "").strip()

        if not final_reply or final_reply.strip() == "":
            final_reply = "æŠ±æ­‰ï¼Œæˆ‘æ­£åœ¨æ€è€ƒä¸­ï¼Œä½†æš«æ™‚ç„¡æ³•ç”¢ç”Ÿå›æ‡‰ã€‚è«‹å†è©¦ä¸€æ¬¡æˆ–æä¾›æ›´å¤šè³‡è¨Šã€‚"

    except Exception as e:
        print(f"ç”ŸæˆéŒ¯èª¤: {e}")
        final_reply = "æŠ±æ­‰ï¼Œç³»çµ±ç™¼ç”ŸéŒ¯èª¤ã€‚"

    # å›è¦†è¨Šæ¯
    line_bot_api.reply_message(
        event.reply_token,
        TextSendMessage(text=final_reply)
    )